\lab{Numerical Derivatives}{Numerical Derivatives}
\label{lab:NumericalDerivatives}
\objective{Understand and implement finite difference approximations of the derivative in single and multiple dimensions. Evaluate the accuracy of these approximations.}

\section*{Derivative Approximations in One Dimension} % =======================

The derivative of a function $f$ at a point $x_0$ is

\begin{equation}
\label{eqn:deriv}
f'(x_0) = \lim_{h\rightarrow 0} \frac{f(x_0 + h)-f(x_0)}{h}.
\end{equation}
%
In this lab, we will investigate one way a computer can calculate $f'(x_0)$.

\subsection*{Forward Difference Quotient} % -----------------------------------

Suppose that in Equation \eqref{eqn:deriv}, instead of taking a limit, we just pick a small value for $h$.
Then we would expect $f'(x_0)$ to be close to the quantity
%
\begin{equation}\label{equ:forward_diff}
\frac{f(x_0 + h)-f(x_0)}{h}.
\end{equation}
%
This quotient is called the \emph{first order forward difference approximation} of the derivative.
Because $f'(x_0)$ is the limit of such quotients, we expect that when $h$ is small, this quotient is close to $f'(x_0)$.
We can use Taylor's formula to find just how close.

By Taylor's formula,
\[
f(x_0+h) = f(x_0) + f'(x_0)h + R_2(h),
\]
where $R_2(h) = \left( \int_0^1 (1-t) f''(x_0+th) dt \right) h^2$.
(This is called the \emph{integral form} of the remainder for Taylor's Theorem; see Volume 1 Chapter 6). When we solve this equation for $f'(x_0)$, we get
%
\begin{equation}\label{equ:forward_diff_with_remainder}
f'(x_0) = \frac{f(x_0+h)-f(x_0)}{h} - \frac{R_2(h)}{h}.
\end{equation}
%
Thus, the error in using the first order forward difference quotient to approximate $f'(x_0)$ is
\[
\left | \frac{R_2(h)}{h} \right | \leq |h| \int_0^1 |1 - t||f''(x_0+th)|dt.
\]
If we assume $f''$ is continuous, then for any $\delta$, set $M = \sup_{x \in (x_0-\delta, x_0+\delta)} f''(x)$. Then if $|h| < \delta$, we have
\[
\left | \frac{R_2(h)}{h} \right | \leq |h|\int_0^1 M dt = M|h|  {\in}  O(h).
\]
Therefore, the error in using \eqref{equ:forward_diff} to approximate $f'(x_0)$ grows like $h$.

\subsection*{Centered Difference Quotient} % ----------------------------------

In fact, we can approximate $f'(x_0)$ to the second order with another difference quotient, called the \emph{centered difference quotient}.  We begin by trying to find the \emph{backward difference quotient}.
Evaluate Taylor's formula at $x_0-h$ to derive
%
\begin{equation}\label{equ:backwards_diff_with_remainder}
f'(x_0) = \frac{f(x_0)-f(x_0-h)}{h} + \frac{R_2(-h)}{h}.
\end{equation}
%
The first term on the right hand side of \eqref{equ:backwards_diff_with_remainder} is called the \emph{backward difference quotient.}
This quotient also approximates $f'(x_0)$ to first order, so it is not the quotient we are looking for.
When we add \eqref{equ:forward_diff_with_remainder} and \eqref{equ:backwards_diff_with_remainder} and solve for $f'(x_0)$ (by dividing by 2), we get
%
\begin{equation}\label{equ:centered_diff}
f'(x_0) = \frac{\frac{1}{2}f(x_0+h) - \frac{1}{2}f(x_0-h)}{h} + \frac{R_2(-h) - R_2(h)}{2h}
\end{equation}
%
The \emph{centered difference quotient} is the first term of the right hand side of \eqref{equ:centered_diff}.
Let us investigate the remainder term to see how accurate this approximation is.
Recall from the proof of Taylor's theorem that $R_k = \frac{f^{(k)}(x_0)}{k!}h^k + R_{k+1}$. Therefore,
%
\begin{align*}
\frac{R_2(-h) - R_2(h)}{2h} &= \frac{1}{2h}\left(\frac{f''(x_0)}{2}h^2 + R_{3}(-h) - \frac{f''(x_0)}{2}h^2 - R_{3}(h) \right)\\
&= \frac{1}{2h} ( R_3(-h)-R_3(h))\\
&= \frac{1}{2h}\left(  \left( \int_0^1 \frac{(1-t)^2}{2} f'''(x_0+th) dt \right) h^3  -  \left(\int_0^1 \frac{(1-t)^2}{2} f'''(x_0-th) dt \right) h^3  \right)\\
&= \left(  \int_0^1 \frac{(1-t)^2}{4}( f'''(x_0+th)-f'''(x_0-th)) \right)h^2\\
&{\in}   O(h^2)
\end{align*}
%
once we restrict $h$ to some $\delta$-neighborhood of 0.
So the error in using the centered difference quotient to approximate $f'(x_0)$ grows like $h^2$, which is smaller than $h$ when $|h|<1$.

\subsection*{Accuracy of Approximations} % ------------------------------------

Let us discuss what step size $h$ we should plug into the difference quotients to get the best approximation to $f'(x_0)$.
Since $f'$ is defined as a limit as $h \rightarrow 0$, you may think that it is best to choose $h$ as small as possible, but this is not the case.
In fact, dividing by very small numbers causes errors in floating point arithmetic.
This means that as we decrease $|h|$, the error between $f'(x_0)$ and the difference quotient will first decrease, but then increase when $|h|$ gets too small because of floating point arithmetic.

Here is an example with the function $f(x) = e^x$. A quick way to write $f$ as a function in Python is with the \li{lambda} keyword.

\begin{lstlisting}
>>> import numpy as np
>>> from matplotlib import pyplot as plt
>>> f = lambda x: np.exp(x)
\end{lstlisting}

In general, the line \li{f = lambda <params> : <expression>} is equivalent to defining a function \li{f} that accepts the parameters \li{params} and returns \li{expression}.

Next we fix a step size \li{h} and define an approximation to the derivative of \li{f} using the \emph{centered difference quotient}.

\begin{lstlisting}
>>> h = 1e-1
>>> Df_app = lambda x: .5*(f(x+h)-f(x-h))/h
\end{lstlisting}

Finally, we check the accuracy of this approximation at $x_0=1$ by computing the difference between \li{Df_app(1)} and the actual derivative evaluated at 1.

\begin{lstlisting}
# Since f(x) = e^x, the derivative of f(x) is f(x)
>>> np.abs( f(1)-Df_app(1) )
0.0045327354883726301
\end{lstlisting}

We note that our functions \li{f} and \li{Df_app} behave as expected when they are passed a NumPy array.

\begin{lstlisting}
>>> h = np.array([1e-1, 1e-3, 1e-5, 1e-7, 1e-9, 1e-11])
>>> np.abs( f(1)-Df_app(1) )
array([  4.53273549e-03,   4.53046679e-07,   5.85869131e-11,
         5.85873572e-11,   6.60275079e-09,   1.04294937e-05])
\end{lstlisting}

These results are summarized in Table \ref{table:approx_errors}.

\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
    $h$     & 1e-1 & 1e-3 & 1e-5  & 1e-7  & 1e-9 & 1e-11 \\ \hline
    Error   & 5e-3 & 5e-7 & 6e-11 & 6e-11 & 7e-9 & 1e-5  \\ \hline
\end{tabular}
\caption{This table shows that it is best not to choose $h$ too small when you approximate derivatives with difference quotients.
Here, ``Error'' equals the absolute value of $f'(1)-f_{app}(1)$ where $f(x) = e^x$ and $f_{app}$ is the centered difference approximation to $f'$.}
\label{table:approx_errors}
\end{center}
\end{table}

Thus, the optimal value of $h$ is one that is small, but not too small. A good choice is \li{h = 1e-5}.

\begin{problem}
Write a function that accepts as input a callable function object \li{f}, an array of points \li{pts}, and
a keyword argument \li{h} that defaults to \li{1e-5}.
Return an array of the \emph{centered difference quotients} of \li{f} at each point in \li{pts} with the specified value of \li{h}.
\end{problem}

\begin{problem}
Write a function that accepts as input a callable function object \li{f}, the derivative \li{df} of the function \li{f}, an array of points \li{pts}, and
a keyword argument \li{h} that defaults to \li{1e-5}.  Return an array of the errors for the \emph{centered difference quotients} at each point in \li{pts} with the specified value of \li{h}.
\end{problem}

\begin{problem}
Use the \emph{centered difference quotient} to approximate the derivative of $f(x) = (\sin(x)+1)^x$ at $x = \frac{\pi}{3}, \frac{\pi}{4},$ and $\frac{\pi}{6}$.  Calculate the error of the approximations.
\end{problem}

You may wonder if the forward or backward difference quotients are ever used, since the centered difference quotient is a more accurate approximation of the derivative.
In fact, there are some functions that in practice do not behave well under centered difference quotients.
In these cases, one must use the forward or backward difference quotient.

Finally, we remark that forward, backward, and centered difference quotients can be used to approximate higher-order derivatives of $f$.
However, taking derivatives is an \emph{unstable} operation.
This means that taking a derivative can amplify the arithmetic error in your computation.
For this reason, difference quotients are not generally used  to approximate derivatives higher than second order.

\begin{problem}
The radar stations A and B, separated by the distance a = 500 m, track the plane C by recording the angles $\alpha$ and $\beta$ at one-second intervals (See figure \ref{plane} ). 
Three successive readings are give in table \ref{angles}.
Use central difference quotients to calculate the speed v of the plane and the climb angle $\gamma$ at t = 10 s. 
The coordinates of the plane can be shown to be
\begin{equation}
x = a \frac{\tan(\beta)}{\tan(\beta)-\tan(\alpha)}
\end{equation}

\begin{equation}
 y = a \frac{\tan(\beta)\tan(\alpha)}{\tan(\beta)-\tan(\alpha)}
\end{equation}


(Kiusalaas, Jaan. Numerical Methods in Engineering with Python 3)
\end{problem}

\begin{figure}
\includegraphics[width=.5\textwidth]{plane_diagram.png}
\caption{Radar stations in relation to plane}
\label{plane}
\end{figure}

\begin{figure}
\includegraphics[width=.5\textwidth]{angles_table.png}
\caption{Angles $\alpha$ and $\beta$ at one-second intervals}
\label{angles}
\end{figure}

\section*{Derivative Approximations in Multiple Dimensions} % =================

Finite difference methods can also be used to calculate derivatives in higher dimensions.
Recall that the Jacobian of a function $f:\mathbb{R}^n \rightarrow \mathbb{R}^m$ at a point $x_0 \in \mathbb{R}^n$ is the $m \times n$ matrix $J = (J_{ij})$ defined component-wise by
\begin{equation*}
J_{ij} = \frac{\partial f_i}{\partial x_j}(x_0).
\end{equation*}
For example, the Jacobian for a function $f:\mathbb{R}^3 \rightarrow \mathbb{R}^2$ is defined by

\[
J = \begin{pmatrix}
\frac{\partial f}{\partial x_1}&\frac{\partial f}{\partial x_2}&\frac{\partial f}{\partial x_3}
\end{pmatrix}
= \begin{pmatrix}
\frac{\partial f_1}{\partial x_1}&\frac{\partial f_1}{\partial x_2}&\frac{\partial f_1}{\partial x_3}\\
\frac{\partial f_2}{\partial x_1}&\frac{\partial f_2}{\partial x_2}&\frac{\partial f_2}{\partial x_3}
\end{pmatrix}.
\]

The Jacobian is useful in many applications.  For example, the Jacobian can be used to find zeros of functions in multiple variables.

The forward difference quotient for approximating a partial derivative is
\begin{equation*}
\frac{\partial f}{\partial x_j} (x_0) \approx \frac{f(x_0+h e_j)-f(x_0)}{h},
\end{equation*}
where $e_j$ is the $j^{th}$ standard basis vector.
Similarly, the centered difference approximation is
\begin{equation*}
\frac{\partial f}{\partial x_j} (x_0) \approx \frac{\frac{1}{2}f(x_0+h e_j)-\frac{1}{2}f(x_0-h e_j)}{h}.
\end{equation*}

\begin{problem}
\leavevmode
Write a function that accepts
\begin{enumerate}
\item a function handle \li{f},
\item an integer \li{n} that is the dimension of the domain of \li{f},
\item an integer \li{m} that is the dimension of the range of \li{f},
\item an \li{1 x n}-dimensional NumPy array \li{pt} representing a point in $\mathbb{R}^n$, and
\item a keyword argument \li{h} that defaults to \li{1e-5}.
\end{enumerate}
Return the approximate Jacobian matrix of \li{f} at \li{pt} using the centered difference quotient.
\end{problem}
\begin{problem}
\item Let $f: \mathbb{R}^2 \to \mathbb{R}^2$ be defined by
%
\begin{equation*}
f(x, y) =
\left[\begin{array}{c}
e^{x} \sin(y) + y^3 \\
3y - \cos(x)
\end{array}\right]
\end{equation*}
%
Find the error between your Jacobian function and the analytically computed derivative on the square $[-1,1] \times [-1,1]$ using ten thousand grid points (100 per side).
You may apply your Jacobian function to the points one at a time using a double \li{for} loop.  Once you get the error matrix for a given point, calculate the Frobenius norm of this matrix (\li{la.norm} defaults to the Frobenius norm).  This norm will be your total error for that point.
What is the maximum error of your Jacobian function over all points in the square?

Hint: The following code defines the function
$f(x,y) = \left[\begin{array}{c} x^2 \\ x+y \end{array}\right]$.

\begin{lstlisting}
# f accepts a length-2 NumPy array
>>> f = lambda x: np.array([x[0]**2, x[0]+x[1]])
\end{lstlisting}
\end{problem}



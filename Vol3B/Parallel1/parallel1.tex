\lab{Parallel Computing with $\texttt{ipyparallel}$}{Parallel Computing with $\texttt{ipyparallel}$}
\objective{(TODO: Awesome objective coming!)}
\label{lab:parallel1}

\section*{Why Parallel Computing?}
When a single processor takes too long to perform a computationally intensive task, there are two simple solutions.
The first is simply to build a faster processor.
Unfortunately, physics gets in the way.
In particular, the problem of heat dissipation has kept processor speeds from increasing as quickly in recent years as they did in the past.
The second solution is to have mutiple processors work together on the same task.
This is the main idea behind parallel computing. Essentially, a `supercomputer' is made up of many normal computers, each with its own memory.

Today, high computing performance is achieved using many processors.
These processors communicate with each other and coordinate their tasks with a message passing system. The details of this message passing system, MPI, will be the topic of the next lab.

In this lab, we will become familiar with some of the basic ideas behind parallel computing.

\section*{Serial Execution vs. Parallel Execution}
Up to this point, all the programs you have written run in \emph{serial}. This means that each command is executed one line at a time. The following exercise will help you visualize what is happening.

\begin{problem}
If you are working on a Linux computer, open a terminal and execute the \li{htop} command. (If \li{htop} is not on your system, install it using your default package manager). When opening this program, your terminal should see an interface similar to Figure \ref{fig:htop}. The numbered bars at the top represent each of the cores of your processor and the workload on each of these cores.

Now, run the following python code with your terminal running \li{htop} still visible. The sole purpose of the following code is to create a computationally intensive function that runs for about 15 seconds.

\begin{lstlisting}
import numpy as np
for i xrange(10000):
    np.random.random(100000)
\end{lstlisting}

You should have seen one of the cores get maxed out at 100\%. It is also possible that you saw the load-carrying core switch midway through the execution of the file. This is evidence one indicator that our script is being executed in serial -- one line at a time, one core at a time.
\end{problem}

\begin{figure}
    \includegraphics[width=\textwidth]{active.jpg}
\caption{An example of \li{htop} with a computationally intense python script running.}
\label{fig:htop}
\end{figure}

As you saw in the exercise above, only one of the cores was carrying the load at a time. This means that we are only using a fraction of the computer's resources. When working on a personal computer, this would often be to your benefit. Dividing jobs among multiple cores is part of what makes smooth multitasking possible. However, in the event you wish to devote all the computer's resources to executing your code, we employ the help of the \li{ipyparallel} module. In theory, you can make your code run $N$ times faster when executing in parallel where $N$ is the number of cores

\section*{The $\texttt{ipyparallel}$ Module}
We will begin our discussion on parallel computing by learning about the \li{ipyparallel} module. Even though this may not be the fastest parallel computing framework available, it is very easy to take advantage of all the cores on your computer with relatively little code. As mentioned before, you cannot expect your code to magically run faster if you requests the usage of more cores on your computer. You have to specify what happens on each core. This concept is key to understanding parallel computing.

\subsection*{Installation and Initialization of $\texttt{ipyparallel}$}

If you have not already installed \li{ipyparallel}, you may do so using the conda package manager.

\begin{lstlisting}
$ conda update conda
$ conda update anaconda
$ conda install ipyparallel
\end{lstlisting}

With \li{ipyparallel} installed, we can now initialize an IPython cluster. We won't go too much into the architecture of the IPython cluster, but if you are interested in learning more, visit \url{https://ipyparallel.readthedocs.io/en/latest/intro.html#architecture-overview}.

Now to initialize an IPython cluster, run the following code:

\begin{lstlisting}
$ ipcluster start
\end{lstlisting}

This will start a cluster with one engine per processor. If you would like to specify the number of engines to initialize, run the following:

\begin{lstlisting}
# start a cluster with 8 engines.
$ ipcluster start --n 8
\end{lstlisting}

If you choose to explicitly specify the number of engines, it is not optimal to initialize more engines than you have processors. Doing so would require multitasking on each processor instead of having each processor dedicated to one task.

If you are more accustomed to using Jupyter Notebooks, you may have noticed the ``Clusters" tab. You can start an IPython cluster in this tab after enabling the \li{ipcluster} notebook extension.

\begin{lstlisting}
$ ipcluster nbextension enable
\end{lstlisting}

\begin{problem}
Initialize an IPython cluster with an engine for each processor. As you did in the previous problem, open \li{htop}. Run the following code and examine what happens in htop.

\begin{lstlisting}
from ipyparallel import Client
client = Client()
dview = client[:]

dview.execute("""
import numpy as np
for i in xrange(10000):
    np.random.random(100000)
""")
\end{lstlisting}

The output of \li{htop} should appear similar to Figure \ref{fig:htop_cluster}. Notice that all of the processors are being utilized to run the script.
\end{problem}

\begin{figure}
    \includegraphics[width=\textwidth]{cluster_active.jpg}
\caption{An example of \li{htop} with a computationally intense python script running in parallel.}
\label{fig:htop_cluster}
\end{figure}

We have now ensured that our IPython cluster is successfully executing on all engines. Now we will dive into the details of how we can utilize the cluster and discuss the syntax to use.

\section*{Syntax for $\texttt{ipyparallel}$}
The basic framework for \li{ipyparallel} revolves around a \li{DirectView} or a \li{LoadBalancedView}.
A \li{DirectView} is the object through which we communicate with each of the engines. You have control over which variables are pushed to each engine and what functions are performed. A \li{LoadBalancedView} takes the commands that are being executed and does its best to distribute the load evenly across all the engines.

For the purposes of having more control over each engine, we will focus on the \li{DirectView} in this lab. To initialize a \li{DirectView}, run the following code:

\begin{lstlisting}
>>> from ipyparallel import Client
>>> client = Client()

# verify the dview has been generated correctly.
#   On my system, I have 4 processors.
>>> client.ids
[0, 1, 2, 3]

# initialize DirectView
>>> dview = client[:]
\end{lstlisting}

\subsection*{Variables on Different Engines}
When using multiple processors, each engine has its own namespace. Therefore, just because a variable has been initialized does not mean that each engine is aware of that variable. You must initialize these variables explicitly on each engine. There are a few different ways to do this.

\begin{lstlisting}
# To share the variable `a' across all engines
>>> a = 10
>>> b = 5
>>> dview["a"] = a
>>> dview["b"] = b

# Or alternatively,
>>> dview.push({'a':a, 'b':b})

# To ensure the variables are on engine 0
>>> client[0]['a']
10

>>> client[0]['b']
5
\end{lstlisting}

The code you just ran is the easiest way to get individiual values on each of the engines. We will discuss some other methods a bit later in this lab. We will now move on to doing some simple computations in parallel.

\subsection*{The $\texttt{apply()}$ and $\texttt{apply\_sync()}$ Methods}
To execute functions on each of the engines, we can use the \li{apply()} or the \li{apply\_sync()} methods.  The difference between \li{apply()} and \li{apply\_sync()} comes down to blocking. If a function is executed with blocking, you will be unable to run any other commands until the function has finished. If a function is run without blocking, you can execute other commands while that function is still computing its result. The \li{apply()} methods executes without blocking and the \li{apply\_sync()} method executes with blocking. The following code box describes how this is done:

\begin{lstlisting}
>>> def add():
...     return a+b

# Runs add() without blocking (in background)
>>> result = dview.apply(add)

# Checks to see if the output is ready
>>> result.ready()
True

# Retrieves output
>>> result.get()
[15, 15, 15, 15]

# Runs add() with blocking
>>> dview.apply_sync(add)
[15, 15, 15, 15]
\end{lstlisting}

You can also pass variables into your function as part of call to \li{apply\_sync()}. Consider the following example:

\begin{lstlisting}
>>> def add(x, y):
...    return x+y

>>> dview.apply_sync(add, 3, 6)
[9, 9, 9, 9]
\end{lstlisting}

\begin{problem}
To this point, the examples of what you can do with parallel computing may not have been very convincing since we were just getting the same answer on each engine. In this problem, we will make better use of each of the engines.

Using \li{apply\_sync()}, draw 1,000,000 samples from a standard normal distribution. Report the mean, max, and min for draws on each individual processor. For example, you output should look like:
\begin{lstlisting}
means = [0.0031776784, -0.0058112042, 0.0012574772, -0.0059655951]
maxs = [4.0388107, 4.3664958, 4.2060184, 4.3391623]
mins = [-4.1508589, -4.3848019, -4.1313324, -4.2826519]
\end{lstlisting}

In theory, using parallel computing for this problem should be approximately $n$ times faster where $n$ is the number of engines you are using.
\end{problem}

\begin{problem}
Now let's do a problem that is a bit more computationally intensive. Define the random variable $X$ to be the maximum out of a $N$ draws from the standard normal distribution. Take 500,000 draws from this distribution and plot the draws in a histogram. The resulting histogram will approximate the p.d.f. of $X$.

Right your function in such a way that each engine will carry an equal load. Also right your function in such a way that it is flexible to the number of engines that are running. HINT: Remember that you can get a list of all available engines using \li{clients.ids}.
\end{problem}
